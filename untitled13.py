# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NnDFC10KiH4Ay8aajJ1zVjpdUcYB3sY2
"""

pip install numpy pandas matplotlib seaborn scipy

import yfinance as yf
import pandas as pd
import numpy as np

class FinancialDataLoader:
    def __init__(self):
        self.data = {}

    def load_data(self, tickers, start_date, end_date):
        for ticker in tickers:
            data = yf.download(ticker, start=start_date, end=end_date)
            self.data[ticker] = data
        return self.data

    def calculate_returns(self, price_series):
        return np.log(price_series / price_series.shift(1)).dropna()

# hmm_basic.py
import numpy as np
from scipy import stats
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BasicHMM:
    def __init__(self, n_states=2):
        self.n_states = n_states
        self.transition_matrix = None
        self.means = None
        self.variances = None
        self.initial_probs = None
        self.log_likelihood_history = []

    def initialize_parameters(self, data):
        """Random initialization of HMM parameters"""
        n = len(data)
        # Transition matrix - each row should sum to 1
        self.transition_matrix = np.random.dirichlet(
            np.ones(self.n_states), size=self.n_states
        )
        # Initialize means spread across data range
        self.means = np.linspace(data.min(), data.max(), self.n_states)
        # Initialize variances based on overall variance
        self.variances = np.ones(self.n_states) * data.var()
        # Uniform initial state probabilities
        self.initial_probs = np.ones(self.n_states) / self.n_states

    def _compute_emission_probabilities(self, data):
        """Compute emission probabilities for each state and observation"""
        n_observations = len(data)
        emission_probs = np.zeros((n_observations, self.n_states))

        for state in range(self.n_states):
            # Gaussian emission probabilities
            emission_probs[:, state] = stats.norm.pdf(
                data, loc=self.means[state], scale=np.sqrt(self.variances[state])
            )

            # Avoid numerical underflow by replacing very small values
            emission_probs[emission_probs[:, state] < 1e-20, state] = 1e-20

        return emission_probs

    def forward_backward(self, data):
        """
        Implement forward-backward algorithm
        Returns: (alpha, beta, gamma, xi, log_likelihood)
        """
        n_observations = len(data)
        emission_probs = self._compute_emission_probabilities(data)

        # Forward pass (alpha)
        alpha = np.zeros((n_observations, self.n_states))
        scale_factors = np.zeros(n_observations)

        # Initialize alpha
        alpha[0] = self.initial_probs * emission_probs[0]
        scale_factors[0] = np.sum(alpha[0])
        alpha[0] /= scale_factors[0]  # Scale to prevent underflow

        # Forward recursion
        for t in range(1, n_observations):
            for j in range(self.n_states):
                alpha[t, j] = emission_probs[t, j] * np.sum(
                    alpha[t-1] * self.transition_matrix[:, j]
                )
            scale_factors[t] = np.sum(alpha[t])
            alpha[t] /= scale_factors[t]  # Scaling

        # Compute log likelihood
        log_likelihood = np.sum(np.log(scale_factors))

        # Backward pass (beta)
        beta = np.zeros((n_observations, self.n_states))
        beta[-1] = 1.0  # Initialize last time step

        # Backward recursion
        for t in range(n_observations-2, -1, -1):
            for i in range(self.n_states):
                beta[t, i] = np.sum(
                    beta[t+1] * emission_probs[t+1] * self.transition_matrix[i, :]
                )
            beta[t] /= scale_factors[t]  # Use same scale factors

        # Compute gamma (state probabilities at each time)
        gamma = alpha * beta
        # Normalize gamma
        gamma_sum = np.sum(gamma, axis=1, keepdims=True)
        gamma = gamma / gamma_sum

        # Compute xi (transition probabilities between consecutive time steps)
        xi = np.zeros((n_observations-1, self.n_states, self.n_states))
        for t in range(n_observations-1):
            for i in range(self.n_states):
                for j in range(self.n_states):
                    xi[t, i, j] = (
                        alpha[t, i] *
                        self.transition_matrix[i, j] *
                        emission_probs[t+1, j] *
                        beta[t+1, j]
                    )
            # Normalize xi for this time step
            xi[t] /= np.sum(xi[t])

        return alpha, beta, gamma, xi, log_likelihood

    def baum_welch(self, data, max_iter=100, tolerance=1e-6):
        """
        EM algorithm for HMM parameter estimation
        """
        n_observations = len(data)
        prev_log_likelihood = -float('inf')

        # Initialize parameters if not already done
        if self.transition_matrix is None:
            self.initialize_parameters(data)

        logger.info("Starting Baum-Welch algorithm...")

        for iteration in range(max_iter):
            # E-step: Forward-backward algorithm
            alpha, beta, gamma, xi, log_likelihood = self.forward_backward(data)

            # Store log likelihood for convergence check
            self.log_likelihood_history.append(log_likelihood)

            # Check convergence
            likelihood_change = log_likelihood - prev_log_likelihood
            if iteration > 0 and abs(likelihood_change) < tolerance:
                logger.info(f"Converged after {iteration} iterations")
                break

            prev_log_likelihood = log_likelihood

            # M-step: Update parameters

            # Update initial probabilities
            self.initial_probs = gamma[0] / np.sum(gamma[0])

            # Update transition matrix
            for i in range(self.n_states):
                for j in range(self.n_states):
                    numerator = np.sum(xi[:, i, j])
                    denominator = np.sum(gamma[:-1, i])
                    if denominator > 0:
                        self.transition_matrix[i, j] = numerator / denominator
                    else:
                        self.transition_matrix[i, j] = 1.0 / self.n_states

            # Ensure transition matrix rows sum to 1
            self.transition_matrix = self.transition_matrix / \
                                   self.transition_matrix.sum(axis=1, keepdims=True)

            # Update means
            for j in range(self.n_states):
                numerator = np.sum(gamma[:, j] * data)
                denominator = np.sum(gamma[:, j])
                if denominator > 0:
                    self.means[j] = numerator / denominator
                else:
                    self.means[j] = np.mean(data)

            # Update variances
            for j in range(self.n_states):
                numerator = np.sum(gamma[:, j] * (data - self.means[j])**2)
                denominator = np.sum(gamma[:, j])
                if denominator > 0:
                    self.variances[j] = numerator / denominator
                else:
                    self.variances[j] = np.var(data)

                # Ensure variance is positive
                self.variances[j] = max(self.variances[j], 1e-6)

            if iteration % 10 == 0:
                logger.info(f"Iteration {iteration}, Log Likelihood: {log_likelihood:.4f}")

        logger.info(f"Final log likelihood: {log_likelihood:.4f}")
        return gamma, log_likelihood

    def viterbi(self, data):
        """
        Viterbi algorithm for finding the most likely state sequence
        """
        n_observations = len(data)
        emission_probs = self._compute_emission_probabilities(data)

        # Initialize Viterbi matrices
        delta = np.zeros((n_observations, self.n_states))
        psi = np.zeros((n_observations, self.n_states), dtype=int)

        # Initialization
        delta[0] = self.initial_probs * emission_probs[0]
        delta[0] /= np.sum(delta[0])  # Scale

        # Recursion
        for t in range(1, n_observations):
            for j in range(self.n_states):
                probabilities = delta[t-1] * self.transition_matrix[:, j]
                psi[t, j] = np.argmax(probabilities)
                delta[t, j] = probabilities[psi[t, j]] * emission_probs[t, j]

            # Scaling to prevent underflow
            delta[t] /= np.sum(delta[t])

        # Backtracking
        states = np.zeros(n_observations, dtype=int)
        states[-1] = np.argmax(delta[-1])

        for t in range(n_observations-2, -1, -1):
            states[t] = psi[t+1, states[t+1]]

        return states

    def predict_regimes(self, data):
        """Predict the most likely regime sequence using Viterbi algorithm"""
        return self.viterbi(data)

    def get_parameters(self):
        """Return current model parameters"""
        return {
            'transition_matrix': self.transition_matrix,
            'means': self.means,
            'variances': self.variances,
            'initial_probs': self.initial_probs
        }

    def simulate(self, n_observations):
        """Simulate data from the HMM"""
        if self.transition_matrix is None:
            raise ValueError("Model not trained. Call baum_welch first.")

        states = np.zeros(n_observations, dtype=int)
        observations = np.zeros(n_observations)

        # Initial state
        states[0] = np.random.choice(self.n_states, p=self.initial_probs)
        observations[0] = np.random.normal(
            self.means[states[0]], np.sqrt(self.variances[states[0]])
        )

        # Generate sequence
        for t in range(1, n_observations):
            states[t] = np.random.choice(
                self.n_states, p=self.transition_matrix[states[t-1]]
            )
            observations[t] = np.random.normal(
                self.means[states[t]], np.sqrt(self.variances[states[t]])
            )

        return observations, states

# test_real_stock_hmm.py
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta

def download_stock_data(ticker, start_date, end_date):
    """Download stock data from Yahoo Finance"""
    print(f"Downloading data for {ticker}...")
    stock_data = yf.download(ticker, start=start_date, end=end_date)
    return stock_data

def prepare_features(data):
    """Calculate returns and technical indicators"""
    # Calculate log returns
    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))

    # Calculate rolling volatility (20-day)
    data['Volatility'] = data['Log_Returns'].rolling(window=20).std()

    # Calculate rolling mean return (20-day)
    data['Mean_Return'] = data['Log_Returns'].rolling(window=20).mean()

    # Calculate RSI (14-day)
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    data['RSI'] = 100 - (100 / (1 + rs))

    # Drop NaN values
    data = data.dropna()

    return data

def test_hmm_on_stock_data(ticker='AAPL', years=5):
    """Test HMM on real stock data"""

    # Calculate date range
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365 * years)

    # Download and prepare data
    stock_data = download_stock_data(ticker, start_date, end_date)
    stock_data = prepare_features(stock_data)

    print(f"Data shape: {stock_data.shape}")
    print(f"Date range: {stock_data.index[0]} to {stock_data.index[-1]}")

    # Use log returns as our primary data
    returns = stock_data['Log_Returns'].values
    volatility = stock_data['Volatility'].values
    rsi = stock_data['RSI'].values

    print(f"Returns stats - Mean: {returns.mean():.6f}, Std: {returns.std():.6f}")
    print(f"Volatility stats - Mean: {volatility.mean():.6f}, Std: {volatility.std():.6f}")

    # Test 1: HMM on returns only
    print("\n" + "="*50)
    print("TEST 1: HMM on Returns Only")
    print("="*50)

    hmm_returns = BasicHMM(n_states=2)
    gamma_returns, log_likelihood_returns = hmm_returns.baum_welch(
        returns, max_iter=100, tolerance=1e-6
    )

    predicted_states_returns = hmm_returns.predict_regimes(returns)

    # Analyze regime characteristics
    analyze_regimes(returns, predicted_states_returns, "Returns")

    # Test 2: HMM on volatility only
    print("\n" + "="*50)
    print("TEST 2: HMM on Volatility Only")
    print("="*50)

    hmm_volatility = BasicHMM(n_states=2)
    gamma_vol, log_likelihood_vol = hmm_volatility.baum_welch(
        volatility, max_iter=100, tolerance=1e-6
    )

    predicted_states_vol = hmm_volatility.predict_regimes(volatility)
    analyze_regimes(volatility, predicted_states_vol, "Volatility")

    # Test 3: HMM on normalized combined features
    print("\n" + "="*50)
    print("TEST 3: HMM on Combined Features")
    print("="*50)

    # Combine returns and volatility (normalized)
    combined_features = np.column_stack([
        (returns - returns.mean()) / returns.std(),
        (volatility - volatility.mean()) / volatility.std()
    ])

    # For simplicity, use the first principal component or just sum
    combined_signal = combined_features[:, 0] + combined_features[:, 1]

    hmm_combined = BasicHMM(n_states=2)
    gamma_combined, log_likelihood_combined = hmm_combined.baum_welch(
        combined_signal, max_iter=100, tolerance=1e-6
    )

    predicted_states_combined = hmm_combined.predict_regimes(combined_signal)
    analyze_regimes(combined_signal, predicted_states_combined, "Combined Features")

    # Plot results
    plot_stock_analysis(stock_data, returns, predicted_states_returns,
                       predicted_states_vol, predicted_states_combined,
                       hmm_returns, hmm_volatility, hmm_combined)

    return (stock_data, hmm_returns, hmm_volatility, hmm_combined,
            predicted_states_returns, predicted_states_vol, predicted_states_combined)

def analyze_regimes(data, states, feature_name):
    """Analyze the characteristics of each regime"""
    unique_states = np.unique(states)

    print(f"\n{feature_name} - Regime Analysis:")
    print("-" * 30)

    for state in unique_states:
        state_data = data[states == state]
        state_fraction = len(state_data) / len(data)

        print(f"Regime {state}:")
        print(f"  Fraction of time: {state_fraction:.3f} ({len(state_data)} periods)")
        print(f"  Mean: {state_data.mean():.6f}")
        print(f"  Std: {state_data.std():.6f}")
        print(f"  Min: {state_data.min():.6f}")
        print(f"  Max: {state_data.max():.6f}")

        # Calculate persistence (average duration in this regime)
        state_changes = np.diff(states == state, prepend=False)
        regime_periods = []
        current_period = 0

        for i in range(len(state_changes)):
            if states[i] == state:
                current_period += 1
            elif current_period > 0:
                regime_periods.append(current_period)
                current_period = 0
        if current_period > 0:
            regime_periods.append(current_period)

        if regime_periods:
            avg_duration = np.mean(regime_periods)
            print(f"  Avg duration: {avg_duration:.1f} periods")
        print()

def plot_stock_analysis(stock_data, returns, states_returns, states_vol,
                       states_combined, hmm_returns, hmm_vol, hmm_combined):
    """Create comprehensive plots of the analysis"""

    fig = plt.figure(figsize=(16, 12))

    # Plot 1: Stock price with returns-based regimes
    ax1 = plt.subplot(3, 2, 1)
    plt.plot(stock_data.index, stock_data['Close'], 'k-', linewidth=1, label='Close Price')

    # Color background based on regime
    for i in range(len(states_returns)-1):
        if states_returns[i] == 0:
            plt.axvspan(stock_data.index[i], stock_data.index[i+1],
                       alpha=0.3, color='red', label='Regime 0' if i == 0 else "")
        else:
            plt.axvspan(stock_data.index[i], stock_data.index[i+1],
                       alpha=0.3, color='blue', label='Regime 1' if i == 0 else "")

    plt.title(f'{stock_data.index[0].year}-{stock_data.index[-1].year} Stock Price with Returns Regimes')
    plt.ylabel('Price ($)')
    plt.legend()

    # Plot 2: Returns with regimes
    ax2 = plt.subplot(3, 2, 2)
    colors_returns = ['red' if s == 0 else 'blue' for s in states_returns]
    plt.scatter(stock_data.index, returns, c=colors_returns, s=10, alpha=0.6)
    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    plt.title('Log Returns with HMM Regimes')
    plt.ylabel('Log Returns')

    # Plot 3: Volatility with regimes
    ax3 = plt.subplot(3, 2, 3)
    colors_vol = ['red' if s == 0 else 'blue' for s in states_vol]
    plt.scatter(stock_data.index, stock_data['Volatility'], c=colors_vol, s=10, alpha=0.6)
    plt.title('Volatility with HMM Regimes')
    plt.ylabel('Volatility')

    # Plot 4: RSI
    ax4 = plt.subplot(3, 2, 4)
    plt.plot(stock_data.index, stock_data['RSI'], 'g-', alpha=0.7)
    plt.axhline(y=70, color='r', linestyle='--', alpha=0.5, label='Overbought (70)')
    plt.axhline(y=30, color='g', linestyle='--', alpha=0.5, label='Oversold (30)')
    plt.title('RSI Indicator')
    plt.ylabel('RSI')
    plt.legend()

    # Plot 5: Log likelihood convergence
    ax5 = plt.subplot(3, 2, 5)
    plt.plot(hmm_returns.log_likelihood_history, label='Returns HMM')
    plt.plot(hmm_vol.log_likelihood_history, label='Volatility HMM')
    plt.plot(hmm_combined.log_likelihood_history, label='Combined HMM')
    plt.title('Log Likelihood Convergence')
    plt.xlabel('Iteration')
    plt.ylabel('Log Likelihood')
    plt.legend()

    # Plot 6: Regime comparison
    ax6 = plt.subplot(3, 2, 6)
    plt.plot(states_returns, 'r-', alpha=0.7, label='Returns Regimes')
    plt.plot(states_vol, 'b-', alpha=0.7, label='Volatility Regimes')
    plt.plot(states_combined + 0.1, 'g-', alpha=0.7, label='Combined Regimes (+0.1)')
    plt.title('Regime Comparison')
    plt.ylabel('Regime State')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Print model parameters
    print("\n" + "="*50)
    print("MODEL PARAMETERS SUMMARY")
    print("="*50)

    print("\nReturns HMM Parameters:")
    params_returns = hmm_returns.get_parameters()
    print(f"Means: {params_returns['means']}")
    print(f"Variances: {params_returns['variances']}")
    print(f"Transition Matrix:\n{params_returns['transition_matrix']}")

    print("\nVolatility HMM Parameters:")
    params_vol = hmm_vol.get_parameters()
    print(f"Means: {params_vol['means']}")
    print(f"Variances: {params_vol['variances']}")
    print(f"Transition Matrix:\n{params_vol['transition_matrix']}")

def trading_strategy_backtest(stock_data, states, lookforward_days=5):
    """Simple backtest of a regime-based trading strategy"""

    returns = stock_data['Log_Returns'].values
    prices = stock_data['Close'].values

    # Simple strategy: Buy when in low volatility regime, sell when in high volatility regime
    # Assuming state 0 is low volatility, state 1 is high volatility
    positions = np.zeros(len(states))
    portfolio_returns = np.zeros(len(states))

    # Start with no position
    position = 0  # 0: out of market, 1: long

    for i in range(len(states)-1):
        current_state = states[i]

        # Trading logic
        if current_state == 0 and position == 0:  # Low volatility regime, not positioned
            # Buy
            position = 1
            positions[i] = 1
        elif current_state == 1 and position == 1:  # High volatility regime, positioned
            # Sell
            position = 0
            positions[i] = 0
        else:
            positions[i] = position

        # Calculate portfolio returns
        if position == 1:
            portfolio_returns[i] = returns[i]
        else:
            portfolio_returns[i] = 0

    # Calculate cumulative returns
    buy_hold_returns = np.cumsum(returns)
    strategy_returns = np.cumsum(portfolio_returns)

    # Calculate performance metrics
    total_return_bh = np.exp(np.sum(returns)) - 1
    total_return_strat = np.exp(np.sum(portfolio_returns)) - 1

    volatility_bh = np.std(returns) * np.sqrt(252)
    volatility_strat = np.std(portfolio_returns) * np.sqrt(252)

    sharpe_bh = (np.mean(returns) * 252) / (volatility_bh) if volatility_bh > 0 else 0
    sharpe_strat = (np.mean(portfolio_returns) * 252) / (volatility_strat) if volatility_strat > 0 else 0

    print("\n" + "="*50)
    print("TRADING STRATEGY BACKTEST")
    print("="*50)
    print(f"Buy & Hold Total Return: {total_return_bh:.2%}")
    print(f"Strategy Total Return: {total_return_strat:.2%}")
    print(f"Buy & Hold Sharpe Ratio: {sharpe_bh:.3f}")
    print(f"Strategy Sharpe Ratio: {sharpe_strat:.3f}")
    print(f"Strategy Volatility: {volatility_strat:.2%}")
    print(f"Time in Market: {np.mean(positions):.2%}")

    # Plot performance
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 1, 1)
    plt.plot(stock_data.index, np.exp(buy_hold_returns), 'b-', label='Buy & Hold')
    plt.plot(stock_data.index, np.exp(strategy_returns), 'r-', label='Regime Strategy')
    plt.plot(stock_data.index, positions, 'g-', alpha=0.3, label='Position (Right Scale)')
    plt.title('Strategy Performance vs Buy & Hold')
    plt.ylabel('Cumulative Return')
    plt.legend()

    plt.subplot(2, 1, 2)
    plt.plot(stock_data.index, states, 'k-', linewidth=2)
    plt.title('Regime States Over Time')
    plt.ylabel('Regime State')
    plt.xlabel('Date')

    plt.tight_layout()
    plt.show()

    return portfolio_returns, positions

if __name__ == "__main__":
    # Test on Apple stock
    results = test_hmm_on_stock_data(ticker='AAPL', years=3)

    stock_data, hmm_returns, hmm_vol, hmm_combined, states_returns, states_vol, states_combined = results

    # Backtest trading strategy
    portfolio_returns, positions = trading_strategy_backtest(stock_data, states_returns)

    # Test on multiple stocks
    print("\n" + "="*50)
    print("COMPARISON ACROSS DIFFERENT STOCKS")
    print("="*50)

    stocks = ['MSFT', 'GOOGL', 'TSLA', 'SPY']
    for stock in stocks:
        try:
            print(f"\nTesting {stock}...")
            test_hmm_on_stock_data(ticker=stock, years=2)
        except Exception as e:
            print(f"Error testing {stock}: {e}")

# hsmm.py
import numpy as np
from scipy import stats
from scipy.special import logsumexp
import logging
from numba import jit, prange
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HiddenSemiMarkovModel:
    """
    Optimized Hidden Semi-Markov Model (HSMM) with efficient computation
    """

    def __init__(self, n_states=2, max_duration=20):  # Reduced max_duration
        self.n_states = n_states
        self.max_duration = max_duration
        self.transition_matrix = None
        self.duration_distributions = None
        self.emission_means = None
        self.emission_variances = None
        self.initial_probs = None

    def initialize_parameters(self, data):
        """Fast initialization using k-means like approach"""
        n = len(data)

        # Quick initialization using quantiles
        quantiles = np.quantile(data, np.linspace(0, 1, self.n_states + 1))
        self.emission_means = (quantiles[1:] + quantiles[:-1]) / 2
        self.emission_variances = np.ones(self.n_states) * np.var(data)

        # Initialize transition matrix (no self-transitions)
        self.transition_matrix = np.ones((self.n_states, self.n_states)) / (self.n_states - 1)
        np.fill_diagonal(self.transition_matrix, 0)

        # Initialize duration distributions (Poisson with reasonable means)
        self.duration_distributions = []
        duration_means = np.linspace(5, 15, self.n_states)
        for mean in duration_means:
            durations = np.arange(1, self.max_duration + 1)
            probs = stats.poisson.pmf(durations, mean)
            probs = probs / probs.sum()
            self.duration_distributions.append(probs)

        # Initial probabilities
        self.initial_probs = np.ones(self.n_states) / self.n_states

    @staticmethod
    @jit(nopython=True, fastmath=True)
    def _compute_emission_probs_fast(data, means, variances):
        """Fast emission probability computation"""
        n_observations = len(data)
        n_states = len(means)
        emission_probs = np.zeros((n_observations, n_states))

        for j in range(n_states):
            std = np.sqrt(variances[j])
            for t in range(n_observations):
                # Gaussian PDF approximation
                z = (data[t] - means[j]) / std
                emission_probs[t, j] = np.exp(-0.5 * z * z) / (std * 2.5066282746310002)  # sqrt(2*pi)

        return emission_probs

    def _fast_forward_backward(self, data):
        """Efficient forward-backward using log-space and vectorization"""
        T = len(data)
        emission_probs = self._compute_emission_probs_fast(data, self.emission_means, self.emission_variances)

        # Convert to log space
        log_emission = np.log(emission_probs + 1e-100)
        log_transition = np.log(self.transition_matrix + 1e-100)
        log_initial = np.log(self.initial_probs + 1e-100)
        log_durations = [np.log(d + 1e-100) for d in self.duration_distributions]

        # Initialize
        log_alpha = np.full((T, self.n_states), -np.inf)
        log_beta = np.full((T, self.n_states), -np.inf)

        # Precompute cumulative emission probabilities for efficiency
        cum_emission = np.cumsum(log_emission, axis=0)

        # Simplified forward pass (approximate)
        for j in range(self.n_states):
            # Initial probabilities
            for d in range(1, min(self.max_duration, T) + 1):
                if d <= T:
                    seg_prob = cum_emission[d-1, j]  # Sum of log emissions
                    log_alpha[d-1, j] = log_initial[j] + log_durations[j][d-1] + seg_prob

        # Forward recursion (simplified)
        for t in range(1, T):
            for j in range(self.n_states):
                max_prev = -np.inf
                for i in range(self.n_states):
                    if i != j:
                        # Find best previous state
                        for d in range(1, min(self.max_duration, t) + 1):
                            if t - d >= 0:
                                prob = log_alpha[t-d, i] + log_transition[i, j]
                                if prob > max_prev:
                                    max_prev = prob

                if max_prev > -np.inf:
                    # Use average duration for efficiency
                    avg_duration = np.argmax(self.duration_distributions[j]) + 1
                    duration = min(avg_duration, T - t)
                    if t + duration - 1 < T:
                        seg_prob = cum_emission[t+duration-1, j] - (cum_emission[t-1, j] if t > 0 else 0)
                        log_alpha[t+duration-1, j] = max_prev + log_durations[j][duration-1] + seg_prob

        # Backward pass (simplified)
        log_beta[-1, :] = 0  # log(1)

        for t in range(T-2, -1, -1):
            for i in range(self.n_states):
                max_future = -np.inf
                for j in range(self.n_states):
                    if i != j:
                        for d in range(1, min(self.max_duration, T - t) + 1):
                            if t + d < T:
                                seg_prob = cum_emission[t+d, j] - cum_emission[t, j]
                                prob = log_beta[t+d, j] + log_transition[i, j] + log_durations[j][d-1] + seg_prob
                                if prob > max_future:
                                    max_future = prob

                if max_future > -np.inf:
                    log_beta[t, i] = max_future

        # Compute state probabilities
        log_gamma = log_alpha + log_beta
        # Normalize
        log_gamma = log_gamma - logsumexp(log_gamma, axis=1, keepdims=True)
        gamma = np.exp(log_gamma)

        log_likelihood = np.sum(logsumexp(log_alpha[-1, :]))

        return gamma, log_likelihood

    def fit(self, data, max_iter=20, tolerance=1e-4):  # Reduced iterations
        """
        Fast EM algorithm for HSMM
        """
        T = len(data)
        prev_log_likelihood = -np.inf

        if self.transition_matrix is None:
            self.initialize_parameters(data)

        logger.info("Starting fast HSMM training...")

        for iteration in range(max_iter):
            # E-step: Fast forward-backward
            gamma, log_likelihood = self._fast_forward_backward(data)

            # Check convergence
            if iteration > 0 and abs(log_likelihood - prev_log_likelihood) < tolerance:
                logger.info(f"Converged after {iteration} iterations")
                break

            prev_log_likelihood = log_likelihood

            # M-step: Update parameters

            # Update emission parameters
            for j in range(self.n_states):
                self.emission_means[j] = np.average(data, weights=gamma[:, j])
                self.emission_variances[j] = np.average((data - self.emission_means[j])**2, weights=gamma[:, j])
                self.emission_variances[j] = max(self.emission_variances[j], 1e-6)

            # Update initial probabilities
            self.initial_probs = gamma[0] / np.sum(gamma[0])

            # Simple duration update (approximate)
            self._update_durations_approx(data, gamma)

            if iteration % 5 == 0:
                logger.info(f"Iteration {iteration}, Log Likelihood: {log_likelihood:.4f}")

        logger.info(f"Final log likelihood: {log_likelihood:.4f}")
        return gamma, log_likelihood

    def _update_durations_approx(self, data, gamma):
        """Approximate duration distribution update"""
        T = len(data)

        for j in range(self.n_states):
            # Estimate typical regime duration from state probabilities
            state_changes = np.diff(gamma[:, j] > 0.5, prepend=False)
            regime_starts = np.where(state_changes)[0]

            if len(regime_starts) > 1:
                durations = np.diff(regime_starts)
                if len(durations) > 0:
                    mean_duration = np.mean(durations)
                    mean_duration = max(1, min(mean_duration, self.max_duration))

                    # Update Poisson distribution
                    d_range = np.arange(1, self.max_duration + 1)
                    probs = stats.poisson.pmf(d_range, mean_duration)
                    probs = probs / probs.sum()
                    self.duration_distributions[j] = probs

    def viterbi_decoding(self, data):
        """
        Fast Viterbi algorithm for HSMM
        """
        T = len(data)
        emission_probs = self._compute_emission_probs_fast(data, self.emission_means, self.emission_variances)

        # Initialize
        delta = np.zeros((T, self.n_states))
        psi = np.zeros((T, self.n_states), dtype=int)

        # Use most likely durations for efficiency
        likely_durations = [np.argmax(d) + 1 for d in self.duration_distributions]

        # Initialization
        for j in range(self.n_states):
            d = min(likely_durations[j], T)
            seg_prob = np.prod(emission_probs[:d, j])
            delta[d-1, j] = self.initial_probs[j] * self.duration_distributions[j][d-1] * seg_prob

        # Recursion
        for t in range(1, T):
            for j in range(self.n_states):
                max_prob = 0
                best_prev = 0

                d = min(likely_durations[j], T - t)

                for i in range(self.n_states):
                    if i != j:
                        prob = delta[t-1, i] * self.transition_matrix[i, j]
                        if prob > max_prob:
                            max_prob = prob
                            best_prev = i

                if max_prob > 0 and t + d - 1 < T:
                    seg_prob = np.prod(emission_probs[t:t+d, j])
                    delta[t+d-1, j] = max_prob * self.duration_distributions[j][d-1] * seg_prob
                    psi[t+d-1, j] = best_prev

        # Backtracking
        states = np.zeros(T, dtype=int)
        current_state = np.argmax(delta[-1, :])
        states[-1] = current_state

        t = T - 1
        while t > 0:
            prev_state = psi[t, current_state]
            duration = min(likely_durations[prev_state], t)

            states[t-duration:t] = prev_state
            current_state = prev_state
            t -= duration

        return states

    def predict_regimes(self, data):
        """Predict regimes using fast Viterbi"""
        return self.viterbi_decoding(data)

    def get_parameters(self):
        """Get model parameters"""
        return {
            'transition_matrix': self.transition_matrix,
            'emission_means': self.emission_means,
            'emission_variances': self.emission_variances,
            'initial_probs': self.initial_probs,
            'duration_distributions': self.duration_distributions
        }

    def get_expected_durations(self):
        """Get expected duration for each state"""
        return [np.sum(np.arange(1, self.max_duration + 1) * d) for d in self.duration_distributions]


# Fast test function
def test_fast_hsmm():
    """Quick test of the optimized HSMM"""
    np.random.seed(42)

    # Generate smaller dataset
    n_points = 500  # Reduced from 1000
    data = []

    # Create clear regimes with different durations
    regimes = [
        {'mean': 1.0, 'std': 0.5, 'duration': 15},
        {'mean': -1.0, 'std': 1.5, 'duration': 8},
    ]

    t = 0
    while t < n_points:
        regime = regimes[t % 2]  # Alternate between regimes
        duration = min(regime['duration'], n_points - t)
        segment = np.random.normal(regime['mean'], regime['std'], duration)
        data.extend(segment)
        t += duration

    data = np.array(data[:n_points])

    print(f"Testing optimized HSMM on {len(data)} data points")

    # Create and train HSMM
    hsmm = HiddenSemiMarkovModel(n_states=2, max_duration=20)

    start_time = time.time()
    gamma, log_likelihood = hsmm.fit(data, max_iter=15)
    training_time = time.time() - start_time

    print(f"Training completed in {training_time:.2f} seconds")
    print(f"Final log likelihood: {log_likelihood:.4f}")

    # Predict regimes
    states = hsmm.predict_regimes(data)

    # Get parameters
    params = hsmm.get_parameters()
    expected_durations = hsmm.get_expected_durations()

    print(f"Emission means: {params['emission_means']}")
    print(f"Expected durations: {expected_durations}")

    return hsmm, data, states

def compare_with_hmm(data):
    """Compare with basic HMM for speed"""


    print("\nComparing with basic HMM...")

    # HMM
    start_time = time.time()
    hmm = BasicHMM(n_states=2)
    hmm.initialize_parameters(data)
    hmm_gamma, hmm_ll = hmm.baum_welch(data, max_iter=15)
    hmm_time = time.time() - start_time

    # HSMM
    start_time = time.time()
    hsmm = HiddenSemiMarkovModel(n_states=2)
    hsmm.initialize_parameters(data)
    hsmm_gamma, hsmm_ll = hsmm.fit(data, max_iter=15)
    hsmm_time = time.time() - start_time

    print(f"HMM training time: {hmm_time:.2f}s, Log likelihood: {hmm_ll:.4f}")
    print(f"HSMM training time: {hsmm_time:.2f}s, Log likelihood: {hsmm_ll:.4f}")
    print(f"Speed ratio: {hsmm_time/hmm_time:.2f}x")

    return hmm, hsmm

if __name__ == "__main__":
    import time

    # Quick test
    hsmm, data, states = test_fast_hsmm()

    # Compare with HMM
    hmm, hsmm = compare_with_hmm(data)

    # Simple plot
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 6))
    plt.plot(data, 'k-', alpha=0.7, linewidth=1, label='Data')
    colors = ['red' if s == 0 else 'blue' for s in states]
    plt.scatter(range(len(data)), data, c=colors, s=10, alpha=0.6)
    plt.title('Data with HSMM Regimes')
    plt.show()

# test_hsmm_outputs.py
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf
from datetime import datetime, timedelta
import time

# Import the correct module for linear_assignment
from scipy.optimize import linear_sum_assignment
import warnings
warnings.filterwarnings('ignore')


def test_hsmm_on_synthetic_data():
    """Test HSMM on synthetic data with known regimes"""
    print("="*60)
    print("TEST 1: SYNTHETIC DATA")
    print("="*60)

    np.random.seed(42)

    # Create synthetic data with clear regimes
    n_points = 800
    data = []
    true_regimes = []

    # Define different regimes
    regimes = [
        {'mean': 1.0, 'std': 0.3, 'duration': 20},   # Bull market
        {'mean': -0.5, 'std': 0.8, 'duration': 15},  # Bear market
        {'mean': 0.2, 'std': 0.4, 'duration': 25},   # Sideways
    ]

    # Generate data
    current_regime = 0
    position = 0
    while position < n_points:
        regime = regimes[current_regime]
        duration = min(regime['duration'], n_points - position)

        # Generate regime data
        regime_data = np.random.normal(regime['mean'], regime['std'], duration)
        data.extend(regime_data)
        true_regimes.extend([current_regime] * duration)

        position += duration
        current_regime = (current_regime + 1) % len(regimes)

    data = np.array(data[:n_points])
    true_regimes = np.array(true_regimes[:n_points])

    print(f"Generated {len(data)} data points")
    print(f"True regime distribution: {np.bincount(true_regimes)}")

    # Test HSMM
    print("\n--- Testing HSMM ---")
    start_time = time.time()
    hsmm = HiddenSemiMarkovModel(n_states=3, max_duration=30)
    hsmm_gamma, hsmm_ll = hsmm.fit(data, max_iter=20)
    hsmm_time = time.time() - start_time

    hsmm_states = hsmm.predict_regimes(data)
    hsmm_params = hsmm.get_parameters()
    hsmm_durations = hsmm.get_expected_durations()

    print(f"HSMM training time: {hsmm_time:.2f}s")
    print(f"HSMM log likelihood: {hsmm_ll:.4f}")
    print(f"HSMM emission means (return regimes): {hsmm_params['emission_means']}")
    print(f"HSMM expected durations (days): {hsmm_durations}")
    print(f"HSMM regime distribution: {np.bincount(hsmm_states)}")

    # Test HMM for comparison
    print("\n--- Testing HMM (Comparison) ---")
    start_time = time.time()
    hmm = BasicHMM(n_states=3)
    hmm.initialize_parameters(data)
    hmm_gamma, hmm_ll = hmm.baum_welch(data, max_iter=20)
    hmm_time = time.time() - start_time

    hmm_states = hmm.predict_regimes(data)
    hmm_params = hmm.get_parameters()

    print(f"HMM training time: {hmm_time:.2f}s")
    print(f"HMM log likelihood: {hmm_ll:.4f}")
    print(f"HMM emission means: {hmm_params['means']}")
    print(f"HMM regime distribution: {np.bincount(hmm_states)}")

    # Calculate accuracy (adjusted for label switching)
    hsmm_accuracy = calculate_accuracy(true_regimes, hsmm_states)
    hmm_accuracy = calculate_accuracy(true_regimes, hmm_states)

    print(f"\n--- Accuracy Comparison ---")
    print(f"HSMM accuracy: {hsmm_accuracy:.4f}")
    print(f"HMM accuracy: {hmm_accuracy:.4f}")

    # Plot results
    plot_comparison(data, true_regimes, hsmm_states, hmm_states, "Synthetic Data")

    return hsmm, hmm, data, true_regimes

def test_hsmm_on_stock_data(ticker='AAPL', years=3):
    """Test HSMM on real stock data"""
    print("\n" + "="*60)
    print(f"TEST 2: REAL STOCK DATA - {ticker}")
    print("="*60)

    # Download stock data
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365 * years)

    try:
        stock_data = yf.download(ticker, start=start_date, end=end_date)
        if stock_data.empty:
            print(f"No data found for {ticker}")
            return None, None, None

        print(f"Downloaded {len(stock_data)} days of data")

        # Use log returns
        prices = stock_data['Close'].values
        returns = np.log(prices[1:] / prices[:-1])
        returns = returns[~np.isnan(returns)]  # Remove NaN

        print(f"Returns stats - Mean: {returns.mean():.6f}, Std: {returns.std():.6f}")

        # Test HSMM on returns
        print("\n--- HSMM on Stock Returns ---")
        start_time = time.time()
        hsmm = HiddenSemiMarkovModel(n_states=3, max_duration=30)
        hsmm_gamma, hsmm_ll = hsmm.fit(returns, max_iter=20)
        hsmm_time = time.time() - start_time

        hsmm_states = hsmm.predict_regimes(returns)
        hsmm_params = hsmm.get_parameters()
        hsmm_durations = hsmm.get_expected_durations()

        print(f"HSMM training time: {hsmm_time:.2f}s")
        print(f"HSMM log likelihood: {hsmm_ll:.4f}")
        print(f"HSMM emission means (return regimes): {hsmm_params['emission_means']}")
        print(f"HSMM expected durations (days): {hsmm_durations}")
        print(f"HSMM regime distribution: {np.bincount(hsmm_states)}")

        # Analyze regimes
        analyze_stock_regimes(returns, hsmm_states, stock_data.index[1:], ticker)

        # Compare with HMM
        print("\n--- HMM on Stock Returns (Comparison) ---")
        start_time = time.time()
        hmm = BasicHMM(n_states=3)
        hmm.initialize_parameters(returns)
        hmm_gamma, hmm_ll = hmm.baum_welch(returns, max_iter=20)
        hmm_time = time.time() - start_time

        hmm_states = hmm.predict_regimes(returns)
        hmm_params = hmm.get_parameters()

        print(f"HMM training time: {hmm_time:.2f}s")
        print(f"HMM log likelihood: {hmm_ll:.4f}")
        print(f"HMM emission means: {hmm_params['means']}")
        print(f"HMM regime distribution: {np.bincount(hmm_states)}")

        # Plot stock results
        plot_stock_results(stock_data, returns, hsmm_states, hmm_states, ticker)

        return hsmm, hmm, returns, hsmm_states

    except Exception as e:
        print(f"Error processing {ticker}: {e}")
        return None, None, None, None

def test_hsmm_on_volatility(ticker='SPY', years=5):
    """Test HSMM on volatility regimes"""
    print("\n" + "="*60)
    print(f"TEST 3: VOLATILITY REGIMES - {ticker}")
    print("="*60)

    # Download data
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365 * years)

    try:
        stock_data = yf.download(ticker, start=start_date, end=end_date)
        prices = stock_data['Close'].values

        # Calculate rolling volatility (30-day)
        returns = np.log(prices[1:] / prices[:-1])
        volatility = pd.Series(returns).rolling(window=30).std().dropna().values
        volatility = volatility[~np.isnan(volatility)]

        print(f"Volatility stats - Mean: {volatility.mean():.6f}, Std: {volatility.std():.6f}")

        # Test HSMM on volatility
        print("\n--- HSMM on Volatility ---")
        hsmm_vol = HiddenSemiMarkovModel(n_states=2, max_duration=40)
        hsmm_vol_gamma, hsmm_vol_ll = hsmm_vol.fit(volatility, max_iter=20)
        hsmm_vol_states = hsmm_vol.predict_regimes(volatility)
        hsmm_vol_params = hsmm_vol.get_parameters()
        hsmm_vol_durations = hsmm_vol.get_expected_durations()

        print(f"HSMM log likelihood: {hsmm_vol_ll:.4f}")
        print(f"HSMM volatility regime means: {hsmm_vol_params['emission_means']}")
        print(f"HSMM expected durations (days): {hsmm_vol_durations}")
        print(f"Volatility regime distribution: {np.bincount(hsmm_vol_states)}")

        # Analyze volatility regimes
        analyze_volatility_regimes(volatility, hsmm_vol_states, ticker)

        plot_volatility_results(volatility, hsmm_vol_states, ticker)

        return hsmm_vol, volatility, hsmm_vol_states

    except Exception as e:
        print(f"Error processing volatility for {ticker}: {e}")
        return None, None, None

def calculate_accuracy(true_states, pred_states):
    """Calculate accuracy accounting for label switching"""
    # from sklearn.utils.linear_assignment_ import linear_assignment # Old import
    from scipy.optimize import linear_sum_assignment
    from sklearn.metrics import confusion_matrix

    # Handle label switching by finding best assignment
    cm = confusion_matrix(true_states, pred_states)
    # Use linear_sum_assignment from scipy
    row_ind, col_ind = linear_sum_assignment(-cm)

    total = cm[row_ind, col_ind].sum()

    return total / np.sum(cm)

def analyze_stock_regimes(returns, states, dates, ticker):
    """Analyze financial characteristics of each regime"""
    print(f"\n--- Financial Analysis of {ticker} Regimes ---")

    unique_states = np.unique(states)

    for state in unique_states:
        mask = states == state
        regime_returns = returns[mask]
        regime_dates = dates[mask] if hasattr(dates, '__len__') else None

        print(f"\nRegime {state}:")
        print(f"  Duration: {len(regime_returns)} days ({len(regime_returns)/len(returns)*100:.1f}%)")
        print(f"  Mean Return: {regime_returns.mean():.6f}")
        print(f"  Volatility: {regime_returns.std():.6f}")
        print(f"  Sharpe Ratio: {regime_returns.mean()/regime_returns.std():.4f}" if regime_returns.std() > 0 else "  Sharpe Ratio: undefined")
        print(f"  Min Return: {regime_returns.min():.6f}")
        print(f"  Max Return: {regime_returns.max():.6f}")

        if regime_dates is not None and len(regime_dates) > 0:
            print(f"  Period: {regime_dates[0].date()} to {regime_dates[-1].date()}")

def analyze_volatility_regimes(volatility, states, ticker):
    """Analyze volatility regimes"""
    print(f"\n--- Volatility Regime Analysis for {ticker} ---")

    unique_states = np.unique(states)

    for state in unique_states:
        mask = states == state
        regime_vol = volatility[mask]

        print(f"\nVolatility Regime {state}:")
        print(f"  Duration: {len(regime_vol)} periods")
        print(f"  Mean Volatility: {regime_vol.mean():.6f}")
        print(f"  Volatility STD: {regime_vol.std():.6f}")
        print(f"  Classification: {'High Vol' if regime_vol.mean() > volatility.mean() else 'Low Vol'}")

def plot_comparison(data, true_states, hsmm_states, hmm_states, title):
    """Plot comparison between true states and predictions"""
    fig, axes = plt.subplots(4, 1, figsize=(14, 12))

    # Plot data
    axes[0].plot(data, 'k-', alpha=0.8, linewidth=1)
    axes[0].set_title(f'{title} - Raw Data')
    axes[0].set_ylabel('Value')
    axes[0].grid(True, alpha=0.3)

    # Plot true states
    axes[1].plot(true_states, 'g-', linewidth=2)
    axes[1].set_title('True States')
    axes[1].set_ylabel('State')
    axes[1].set_ylim(-0.5, max(true_states) + 0.5)
    axes[1].grid(True, alpha=0.3)

    # Plot HSMM states
    axes[2].plot(hsmm_states, 'r-', linewidth=2)
    axes[2].set_title('HSMM Predicted States')
    axes[2].set_ylabel('State')
    axes[2].set_ylim(-0.5, max(hsmm_states) + 0.5)
    axes[2].grid(True, alpha=0.3)

    # Plot HMM states
    axes[3].plot(hmm_states, 'b-', linewidth=2)
    axes[3].set_title('HMM Predicted States')
    axes[3].set_ylabel('State')
    axes[3].set_xlabel('Time')
    axes[3].set_ylim(-0.5, max(hmm_states) + 0.5)
    axes[3].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def plot_stock_results(stock_data, returns, hsmm_states, hmm_states, ticker):
    """Plot stock analysis results"""
    fig, axes = plt.subplots(3, 1, figsize=(14, 10))

    # Plot price
    axes[0].plot(stock_data.index, stock_data['Close'], 'b-', linewidth=1.5)
    axes[0].set_title(f'{ticker} Price')
    axes[0].set_ylabel('Price ($)')
    axes[0].grid(True, alpha=0.3)

    # Plot returns with HSMM regimes
    axes[1].plot(stock_data.index[1:], returns, 'k-', alpha=0.7, linewidth=1)
    colors = ['red' if s == 0 else 'blue' if s == 1 else 'green' for s in hsmm_states]
    axes[1].scatter(stock_data.index[1:], returns, c=colors, s=10, alpha=0.6)
    axes[1].set_title('Returns with HSMM Regimes')
    axes[1].set_ylabel('Log Returns')
    axes[1].grid(True, alpha=0.3)

    # Plot regime states comparison
    axes[2].plot(hsmm_states, 'r-', alpha=0.7, label='HSMM States')
    axes[2].plot(hmm_states, 'b-', alpha=0.7, label='HMM States')
    axes[2].set_title('Regime States Comparison')
    axes[2].set_ylabel('State')
    axes[2].set_xlabel('Time')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def plot_volatility_results(volatility, states, ticker):
    """Plot volatility regime results"""
    fig, axes = plt.subplots(2, 1, figsize=(14, 8))

    # Plot volatility
    axes[0].plot(volatility, 'purple', linewidth=1.5)
    axes[0].set_title(f'{ticker} - Rolling Volatility')
    axes[0].set_ylabel('Volatility')
    axes[0].grid(True, alpha=0.3)

    # Plot volatility regimes
    axes[1].plot(states, 'orange', linewidth=2)
    axes[1].set_title('Volatility Regimes (HSMM)')
    axes[1].set_ylabel('Regime')
    axes[1].set_xlabel('Time')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def performance_benchmark():
    """Benchmark performance on different data sizes"""
    print("\n" + "="*60)
    print("PERFORMANCE BENCHMARK")
    print("="*60)

    data_sizes = [100, 500, 1000, 2000]

    for size in data_sizes:
        print(f"\nTesting with {size} data points...")

        # Generate test data
        data = np.random.normal(0, 1, size)

        # HSMM performance
        start_time = time.time()
        hsmm = HiddenSemiMarkovModel(n_states=2)
        hsmm.fit(data, max_iter=10)
        hsmm_time = time.time() - start_time

        # HMM performance
        start_time = time.time()
        hmm = BasicHMM(n_states=2)
        hmm.initialize_parameters(data)
        hmm.baum_welch(data, max_iter=10)
        hmm_time = time.time() - start_time

        print(f"  HSMM time: {hsmm_time:.3f}s")
        print(f"  HMM time: {hmm_time:.3f}s")
        print(f"  Ratio: {hsmm_time/hmm_time:.2f}x")

def main():
    """Run all tests"""
    print("HIDDEN SEMI-MARKOV MODEL TESTING SUITE")
    print("======================================")

    # Test 1: Synthetic data
    hsmm_synth, hmm_synth, data_synth, true_synth = test_hsmm_on_synthetic_data()

    # Test 2: Stock returns
    hsmm_stock, hmm_stock, returns_stock, states_stock = test_hsmm_on_stock_data('AAPL', 3)

    # Test 3: Volatility regimes
    hsmm_vol, volatility, vol_states = test_hsmm_on_volatility('SPY', 5)

    # Performance benchmark
    performance_benchmark()

    print("\n" + "="*60)
    print("TESTING COMPLETE")
    print("="*60)

    # Summary
    print("\nSUMMARY:")
    print("- HSMM provides explicit duration modeling")
    print("- Better for regimes with characteristic persistence")
    print("- More computationally intensive than HMM")
    print("- Useful for financial regime detection")

if __name__ == "__main__":
    import pandas as pd
    from sklearn.metrics import confusion_matrix
    from scipy.optimize import linear_sum_assignment

    main()

# Test on a specific stock
hsmm, hmm, returns, states = test_hsmm_on_stock_data('AAPL', years=3)

# ms_var.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from scipy.optimize import minimize
import logging
from typing import List, Tuple, Dict, Any
import yfinance as yf
from datetime import datetime, timedelta

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MSVARModel:
    """
    Markov-Switching Vector Autoregression Model
    Models multiple time series with regime-dependent parameters
    """

    def __init__(self, n_states=2, lag_order=1, n_variables=2):
        self.n_states = n_states
        self.lag_order = lag_order
        self.n_variables = n_variables
        self.transition_matrix = None
        self.initial_probs = None
        self.parameters = None
        self.regime_probabilities = None
        self.smoothed_regimes = None

    def initialize_parameters(self, data):
        """Initialize MS-VAR parameters"""
        T, n_vars = data.shape

        # Initialize transition matrix
        self.transition_matrix = np.random.dirichlet(
            np.ones(self.n_states) * 10, size=self.n_states
        )

        # Initial state probabilities
        self.initial_probs = np.ones(self.n_states) / self.n_states

        # Initialize VAR parameters for each regime
        self.parameters = {
            'intercepts': np.random.normal(0, 0.1, (self.n_states, n_vars)),
            'coefficients': np.random.normal(0, 0.05, (self.n_states, n_vars, n_vars * self.lag_order)),
            'covariances': np.array([np.eye(n_vars) * 0.1 for _ in range(self.n_states)])
        }

    def _create_lagged_matrix(self, data):
        """Create matrix of lagged variables"""
        T, n_vars = data.shape
        lagged_data = []

        for t in range(self.lag_order, T):
            lagged_vec = data[t-self.lag_order:t].flatten()
            lagged_data.append(lagged_vec)

        return np.array(lagged_data), data[self.lag_order:]

    def _regime_likelihood(self, data, regime):
        """Compute likelihood for a specific regime"""
        lagged_matrix, current_data = self._create_lagged_matrix(data)
        T_lagged, _ = lagged_matrix.shape

        intercepts = self.parameters['intercepts'][regime]
        coefficients = self.parameters['coefficients'][regime]
        covariance = self.parameters['covariances'][regime]

        # Predict using VAR model
        predictions = intercepts + lagged_matrix @ coefficients.T

        # Compute multivariate normal likelihood
        residuals = current_data - predictions
        likelihoods = np.zeros(T_lagged)

        for t in range(T_lagged):
            try:
                likelihoods[t] = stats.multivariate_normal.pdf(
                    residuals[t], mean=np.zeros(self.n_variables), cov=covariance
                )
            except:
                likelihoods[t] = 1e-10  # Avoid numerical issues

        return likelihoods

    def _forward_backward(self, data):
        """Forward-backward algorithm for regime probabilities"""
        T, n_vars = data.shape
        T_effective = T - self.lag_order

        # Compute emission probabilities for each regime
        emission_probs = np.zeros((T_effective, self.n_states))
        for regime in range(self.n_states):
            emission_probs[:, regime] = self._regime_likelihood(data, regime)

        # Avoid numerical issues
        emission_probs = np.clip(emission_probs, 1e-10, 1e10)

        # Forward pass
        alpha = np.zeros((T_effective, self.n_states))
        scale_factors = np.zeros(T_effective)

        # Initialize
        alpha[0] = self.initial_probs * emission_probs[0]
        scale_factors[0] = np.sum(alpha[0])
        alpha[0] /= scale_factors[0]

        # Forward recursion
        for t in range(1, T_effective):
            for j in range(self.n_states):
                alpha[t, j] = emission_probs[t, j] * np.sum(
                    alpha[t-1] * self.transition_matrix[:, j]
                )
            scale_factors[t] = np.sum(alpha[t])
            alpha[t] /= scale_factors[t]

        # Backward pass
        beta = np.zeros((T_effective, self.n_states))
        beta[-1] = 1.0

        for t in range(T_effective-2, -1, -1):
            for i in range(self.n_states):
                beta[t, i] = np.sum(
                    beta[t+1] * emission_probs[t+1] * self.transition_matrix[i, :]
                )
            beta[t] /= scale_factors[t]

        # Smoothed probabilities
        gamma = alpha * beta
        gamma = gamma / np.sum(gamma, axis=1, keepdims=True)

        # Two-slice probabilities
        xi = np.zeros((T_effective-1, self.n_states, self.n_states))
        for t in range(T_effective-1):
            for i in range(self.n_states):
                for j in range(self.n_states):
                    xi[t, i, j] = (
                        alpha[t, i] *
                        self.transition_matrix[i, j] *
                        emission_probs[t+1, j] *
                        beta[t+1, j]
                    )
            xi[t] /= np.sum(xi[t])

        log_likelihood = np.sum(np.log(scale_factors))

        return gamma, xi, log_likelihood

    def _update_parameters(self, data, gamma, xi):
        """Update VAR parameters using EM algorithm"""
        T, n_vars = data.shape
        T_effective = T - self.lag_order

        lagged_matrix, current_data = self._create_lagged_matrix(data)

        for regime in range(self.n_states):
            regime_weights = gamma[:, regime]

            # Weighted regression for VAR parameters
            X = np.column_stack([np.ones(T_effective), lagged_matrix])
            W = np.diag(regime_weights)

            try:
                # Weighted least squares
                coeffs = np.linalg.lstsq(X.T @ W @ X, X.T @ W @ current_data, rcond=None)[0]

                self.parameters['intercepts'][regime] = coeffs[0]
                self.parameters['coefficients'][regime] = coeffs[1:].T

                # Update covariance matrix
                predictions = X @ coeffs
                residuals = current_data - predictions
                weighted_cov = (residuals.T @ (W @ residuals)) / np.sum(regime_weights)

                # Ensure positive definite
                eigenvals, eigenvecs = np.linalg.eigh(weighted_cov)
                eigenvals = np.maximum(eigenvals, 1e-6)  # Ensure positive eigenvalues
                self.parameters['covariances'][regime] = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T

            except np.linalg.LinAlgError:
                logger.warning(f"Regression failed for regime {regime}, keeping previous parameters")

        # Update transition matrix
        for i in range(self.n_states):
            for j in range(self.n_states):
                numerator = np.sum(xi[:, i, j])
                denominator = np.sum(gamma[:-1, i])
                if denominator > 0:
                    self.transition_matrix[i, j] = numerator / denominator

        # Ensure transition matrix rows sum to 1
        self.transition_matrix = self.transition_matrix / self.transition_matrix.sum(axis=1, keepdims=True)

        # Update initial probabilities
        self.initial_probs = gamma[0] / np.sum(gamma[0])

    def fit(self, data, max_iter=30, tolerance=1e-4):
        """
        Fit MS-VAR model using EM algorithm
        data: T x n_variables numpy array
        """
        if isinstance(data, pd.DataFrame):
            data = data.values

        T, n_vars = data.shape
        self.n_variables = n_vars

        # Initialize parameters
        if self.parameters is None:
            self.initialize_parameters(data)

        logger.info(f"Fitting MS-VAR({self.lag_order}) with {self.n_states} regimes")
        logger.info(f"Data shape: {data.shape}")

        prev_log_likelihood = -np.inf
        log_likelihoods = []

        for iteration in range(max_iter):
            # E-step
            gamma, xi, log_likelihood = self._forward_backward(data)

            log_likelihoods.append(log_likelihood)

            # Check convergence
            if iteration > 0 and abs(log_likelihood - prev_log_likelihood) < tolerance:
                logger.info(f"Converged after {iteration} iterations")
                break

            prev_log_likelihood = log_likelihood

            # M-step
            self._update_parameters(data, gamma, xi)

            if iteration % 5 == 0:
                logger.info(f"Iteration {iteration}, Log Likelihood: {log_likelihood:.4f}")

        # Store final regime probabilities
        self.regime_probabilities = gamma
        self.smoothed_regimes = np.argmax(gamma, axis=1)

        logger.info(f"Final log likelihood: {log_likelihood:.4f}")
        return log_likelihoods

    def predict(self, data, steps=1):
        """Predict future values"""
        if isinstance(data, pd.DataFrame):
            data = data.values

        T, n_vars = data.shape
        predictions = []
        current_regime_probs = self.regime_probabilities[-1] if self.regime_probabilities is not None else self.initial_probs

        current_data = data[-self.lag_order:].flatten()

        for step in range(steps):
            # Predict next period for each regime
            regime_predictions = []
            for regime in range(self.n_states):
                intercept = self.parameters['intercepts'][regime]
                coefficients = self.parameters['coefficients'][regime]
                prediction = intercept + coefficients @ current_data
                regime_predictions.append(prediction)

            regime_predictions = np.array(regime_predictions)

            # Weight by regime probabilities
            weighted_prediction = current_regime_probs @ regime_predictions
            predictions.append(weighted_prediction)

            # Update current data for next prediction
            current_data = np.concatenate([current_data[n_vars:], weighted_prediction])

            # Update regime probabilities
            current_regime_probs = current_regime_probs @ self.transition_matrix

        return np.array(predictions)

    def get_regime_characteristics(self, data):
        """Analyze characteristics of each regime"""
        if self.regime_probabilities is None:
            raise ValueError("Model not fitted. Call fit() first.")

        T, n_vars = data.shape
        T_effective = T - self.lag_order
        effective_data = data[self.lag_order:]

        regime_stats = {}

        for regime in range(self.n_states):
            regime_mask = self.smoothed_regimes == regime
            regime_data = effective_data[regime_mask]

            if len(regime_data) > 0:
                regime_stats[f'regime_{regime}'] = {
                    'duration': len(regime_data),
                    'proportion': len(regime_data) / T_effective,
                    'mean_returns': np.mean(regime_data, axis=0),
                    'volatility': np.std(regime_data, axis=0),
                    'correlation': np.corrcoef(regime_data.T) if len(regime_data) > 1 else np.eye(n_vars)
                }

        return regime_stats

    def plot_regimes(self, data, variable_names=None):
        """Plot regime probabilities and data"""
        if self.regime_probabilities is None:
            raise ValueError("Model not fitted. Call fit() first.")

        if isinstance(data, pd.DataFrame):
            variable_names = data.columns.tolist()
            data = data.values

        T, n_vars = data.shape
        T_effective = T - self.lag_order
        effective_data = data[self.lag_order:]

        fig, axes = plt.subplots(2 + n_vars, 1, figsize=(12, 3 * (2 + n_vars)))

        # Plot regime probabilities
        for regime in range(self.n_states):
            axes[0].plot(self.regime_probabilities[:, regime],
                        label=f'Regime {regime}', alpha=0.8)
        axes[0].set_title('Regime Probabilities')
        axes[0].set_ylabel('Probability')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Plot smoothed regimes
        axes[1].plot(self.smoothed_regimes, 'k-', linewidth=2)
        axes[1].set_title('Most Likely Regime')
        axes[1].set_ylabel('Regime')
        axes[1].set_ylim(-0.5, self.n_states - 0.5)
        axes[1].grid(True, alpha=0.3)

        # Plot each variable with regime colors
        for i in range(n_vars):
            var_name = variable_names[i] if variable_names else f'Variable {i}'
            colors = [f'C{regime}' for regime in self.smoothed_regimes]

            scatter = axes[2+i].scatter(range(T_effective), effective_data[:, i],
                                      c=colors, s=10, alpha=0.6)
            axes[2+i].plot(effective_data[:, i], 'k-', alpha=0.3, linewidth=0.5)
            axes[2+i].set_title(f'{var_name} with Regimes')
            axes[2+i].set_ylabel(var_name)
            axes[2+i].set_xlabel('Time') # Added xlabel for clarity
            axes[2+i].grid(True, alpha=0.3)


        plt.tight_layout()
        plt.show()

    def get_parameters(self):
        """Get model parameters"""
        return {
            'transition_matrix': self.transition_matrix,
            'initial_probs': self.initial_probs,
            'parameters': self.parameters,
            'n_states': self.n_states,
            'lag_order': self.lag_order,
            'n_variables': self.n_variables
        }


# Stock-specific MS-VAR implementation
class StockMSVAR:
    """MS-VAR model specialized for stock analysis"""

    def __init__(self, n_states=3, lag_order=1):
        self.msvar = MSVARModel(n_states=n_states, lag_order=lag_order)
        self.ticker_data = {}

    def prepare_stock_data(self, tickers, start_date, end_date, return_type='log'):
        """Prepare stock data for MS-VAR analysis"""
        data = {}
        returns_data = {}

        for ticker in tickers:
            try:
                stock_data = yf.download(ticker, start=start_date, end=end_date)
                if stock_data.empty:
                    logger.warning(f"No data found for {ticker}")
                    continue

                if return_type == 'log':
                    returns = np.log(stock_data['Close'] / stock_data['Close'].shift(1))
                else:  # simple returns
                    returns = stock_data['Close'].pct_change()

                returns = returns.dropna()
                data[ticker] = stock_data
                returns_data[ticker] = returns # Store as pandas Series to preserve index

                logger.info(f"Downloaded {len(returns)} days of data for {ticker}")

            except Exception as e:
                logger.error(f"Error downloading {ticker}: {e}")

        # Combine returns into a single DataFrame using concatenation
        # This ensures the index is preserved
        if not returns_data:
            raise ValueError("No stock data was successfully downloaded.")

        returns_df = pd.concat(returns_data.values(), axis=1, keys=returns_data.keys())
        returns_df = returns_df.dropna()


        self.ticker_data = data
        self.returns_df = returns_df # Corrected attribute name

        return returns_df

    def fit_model(self, returns_df=None, **kwargs):
        """Fit MS-VAR model to stock returns"""
        if returns_df is None:
            returns_df = self.returns_df

        if returns_df is None or returns_df.empty:
            raise ValueError("No returns data available. Call prepare_stock_data first.")

        returns_array = returns_df.values
        log_likelihoods = self.msvar.fit(returns_array, **kwargs)

        return log_likelihoods

    def analyze_regimes(self, returns_df=None):
        """Analyze stock market regimes"""
        if returns_df is None:
            returns_df = self.returns_df

        if returns_df is None or returns_df.empty:
            raise ValueError("No returns data available.")

        regime_stats = self.msvar.get_regime_characteristics(returns_df.values)

        print("Stock Market Regime Analysis:")
        print("=" * 50)

        for regime, stats in regime_stats.items():
            print(f"\n{regime.upper()}:")
            print(f"  Duration: {stats['duration']} days ({stats['proportion']:.1%})")
            print(f"  Mean Returns:")
            for i, ticker in enumerate(returns_df.columns):
                mean_return = stats['mean_returns'][i] * 100  # Convert to percentage
                volatility = stats['volatility'][i] * 100
                print(f"    {ticker}: {mean_return:.4f}% (vol: {volatility:.4f}%)")

        return regime_stats

    def predict_returns(self, steps=5):
        """Predict future returns"""
        if self.returns_df is None or self.returns_df.empty:
            raise ValueError("No returns data available.")

        predictions = self.msvar.predict(self.returns_df.values, steps=steps)

        # Convert predictions to DataFrame
        pred_df = pd.DataFrame(
            predictions,
            columns=[f'Pred_{ticker}' for ticker in self.returns_df.columns]
        )

        return pred_df

    def plot_analysis(self):
        """Comprehensive plot of MS-VAR analysis"""
        if self.msvar.regime_probabilities is None:
            raise ValueError("Model not fitted.")

        if self.returns_df is None or self.returns_df.empty:
             raise ValueError("No returns data available to plot.")

        self.msvar.plot_regimes(self.returns_df, self.returns_df.columns.tolist())

        # Additional plots
        n_regimes = self.msvar.n_states
        n_cols = 2
        n_rows = 1 + (n_regimes + 1) // n_cols  # +1 for the first two plots

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))

        # Handle case when there's only one row
        if n_rows == 1:
            axes = axes.reshape(1, -1)

        # Plot regime persistence (top left)
        im = axes[0, 0].imshow(self.msvar.transition_matrix, cmap='Blues', vmin=0, vmax=1)
        axes[0, 0].set_title('Regime Transition Matrix')
        axes[0, 0].set_xlabel('To Regime')
        axes[0, 0].set_ylabel('From Regime')
        plt.colorbar(im, ax=axes[0, 0])

        # Add values to heatmap
        for i in range(self.msvar.transition_matrix.shape[0]):
            for j in range(self.msvar.transition_matrix.shape[1]):
                axes[0, 0].text(j, i, f'{self.msvar.transition_matrix[i,j]:.3f}',
                              ha='center', va='center', color='black')

        # Regime durations (top right)
        regime_stats = self.analyze_regimes()
        durations = [stats['duration'] for stats in regime_stats.values()]
        regimes = list(regime_stats.keys())

        axes[0, 1].bar(regimes, durations, color=['red', 'blue', 'green'][:len(regimes)])
        axes[0, 1].set_title('Average Regime Duration')
        axes[0, 1].set_ylabel('Days')

        # Correlation structure by regime (bottom row(s))
        for i, (regime, stats) in enumerate(regime_stats.items()):
            row = 1 + i // n_cols
            col = i % n_cols

            if row < n_rows:  # Ensure we don't exceed subplot bounds
                corr_matrix = stats['correlation']
                im = axes[row, col].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)
                axes[row, col].set_title(f'Correlation - {regime}')
                axes[row, col].set_xticks(range(len(self.returns_df.columns)))
                axes[row, col].set_xticklabels(self.returns_df.columns, rotation=45)
                axes[row, col].set_yticks(range(len(self.returns_df.columns)))
                axes[row, col].set_yticklabels(self.returns_df.columns)
                plt.colorbar(im, ax=axes[row, col])

        # Hide any unused subplots
        total_plots_used = 2 + n_regimes
        total_subplots = n_rows * n_cols

        for i in range(total_plots_used, total_subplots):
            row = i // n_cols
            col = i % n_cols
            if row < n_rows:
                axes[row, col].axis('off')

        plt.tight_layout()
        plt.show()


# Test functions
def test_msvar_synthetic():
    """Test MS-VAR on synthetic data"""
    print("Testing MS-VAR on Synthetic Data...")

    np.random.seed(42)

    # Generate synthetic multivariate time series with regimes
    T = 1000
    n_vars = 3

    # Create regime-dependent parameters
    regimes = [
        {'means': [0.1, 0.05, 0.02], 'vols': [0.1, 0.08, 0.05], 'corr': 0.8},   # Bull market
        {'means': [-0.05, -0.02, 0.01], 'vols': [0.2, 0.15, 0.1], 'corr': 0.3}, # Bear market
        {'means': [0.02, 0.01, 0.005], 'vols': [0.05, 0.04, 0.03], 'corr': 0.6} # Sideways
    ]

    data = []
    true_regimes = []

    current_regime = 0
    regime_duration = 0
    min_duration, max_duration = 50, 150

    for t in range(T):
        regime = regimes[current_regime]

        # Generate correlated returns
        cov_matrix = np.eye(n_vars) * np.array(regime['vols'])**2
        cov_matrix += regime['corr'] * (1 - np.eye(n_vars)) * np.outer(regime['vols'], regime['vols'])

        returns = np.random.multivariate_normal(regime['means'], cov_matrix)
        data.append(returns)
        true_regimes.append(current_regime)

        regime_duration += 1
        if regime_duration > min_duration and (regime_duration >= max_duration or np.random.random() < 0.05):
            current_regime = (current_regime + 1) % len(regimes)
            regime_duration = 0

    data = np.array(data)
    true_regimes = np.array(true_regimes)

    print(f"Generated {T} periods with {n_vars} variables")
    print(f"True regime distribution: {np.bincount(true_regimes)}")

    # Fit MS-VAR
    msvar = MSVARModel(n_states=3, lag_order=1, n_variables=3)
    log_likelihoods = msvar.fit(data, max_iter=20)

    # Analyze results
    predicted_regimes = msvar.smoothed_regimes
    accuracy = np.mean(predicted_regimes == true_regimes[1:])  # Account for lag

    print(f"Regime prediction accuracy: {accuracy:.4f}")
    print(f"Final log likelihood: {log_likelihoods[-1]:.4f}")

    # Plot results
    msvar.plot_regimes(data, ['Stock_A', 'Stock_B', 'Stock_C'])

    return msvar, data, true_regimes

def test_stock_msvar():
    """Test MS-VAR on real stock data"""
    print("\nTesting MS-VAR on Real Stock Data...")

    # Define stock portfolio
    tickers = ['AAPL', 'MSFT', 'GOOGL']  # Tech stocks
    # tickers = ['SPY', 'TLT', 'GLD']    # Asset classes

    # Initialize stock MS-VAR
    stock_model = StockMSVAR(n_states=3, lag_order=1)

    # Download and prepare data
    end_date = datetime.now()
    start_date = end_date - timedelta(days=5*365)  # 5 years of data

    returns_df = stock_model.prepare_stock_data(tickers, start_date, end_date)

    print(f"Prepared data with {len(returns_df)} observations")
    print(f"Stocks: {list(returns_df.columns)}")

    # Fit model
    log_likelihoods = stock_model.fit_model(max_iter=25)

    # Analyze regimes
    regime_stats = stock_model.analyze_regimes()

    # Make predictions
    predictions = stock_model.predict_returns(steps=10)
    print(f"\nNext 10-day return predictions:")
    print(predictions)

    # Plot comprehensive analysis
    stock_model.plot_analysis()

    return stock_model, returns_df, regime_stats

if __name__ == "__main__":
    # Test on synthetic data
    msvar_synth, data_synth, true_regimes = test_msvar_synthetic()

    # Test on real stock data
    stock_model, returns_df, regime_stats = test_stock_msvar()